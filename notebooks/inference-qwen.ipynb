{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c18d98-385c-4d43-be6d-9fd601e5b660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822efe1b-388f-4e00-b713-d9a98e35867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MAX_TOKENS = 128\n",
    "\n",
    "# SAVED_MODEL_PATH = 'models/EleutherAI/gpt-neo-125m-512'\n",
    "MODEL_NAME = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "SAVED_MODEL_PATH = f'models/{MODEL_NAME}-{MAX_TOKENS}'\n",
    "\n",
    "\n",
    "student_model = PeftModel.from_pretrained(\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16),\n",
    "    SAVED_MODEL_PATH\n",
    ")\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(SAVED_MODEL_PATH, trust_remote_code=True)\n",
    "student_tokenizer.pad_token = student_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b9a6bd-9d61-42ab-968d-848ed4f1b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional\n",
    "\n",
    "# def chat_with_model(prompt, max_new_tokens=64):\n",
    "#     device = student_model.device\n",
    "#     inputs = student_tokenizer(prompt, return_tensors=\"pt\")\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "#     with torch.inference_mode():\n",
    "#         outputs = student_model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             pad_token_id=student_tokenizer.eos_token_id\n",
    "#         )\n",
    "\n",
    "#     response_ids = outputs[0][input_length:]\n",
    "#     response_text = student_tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "#     return response_text\n",
    "\n",
    "def chat_with_model(prompt, return_confidence=True, max_new_tokens=64):\n",
    "    device = student_model.device\n",
    "    inputs = student_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = student_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=student_tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=return_confidence\n",
    "        )\n",
    "\n",
    "    response_ids = outputs.sequences[0][input_length:]\n",
    "    response_text = student_tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "    if return_confidence:\n",
    "        scores = torch.stack(outputs.scores, dim=0).to(outputs.sequences.device)\n",
    "        probs = functional.softmax(scores, dim=-1)\n",
    "        max_probs = probs.max(dim=-1).values\n",
    "        avg_confidence = max_probs.mean().item()\n",
    "        return response_text, avg_confidence\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4016da-342f-4838-b565-c888c946cddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response'],\n",
       "    num_rows: 30000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv('sample/merged_distilled_dataset.csv')\n",
    "distilled_dataset = Dataset.from_pandas(df)\n",
    "distilled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5187ef-2384-465b-94b4-4981e81334c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: User: when i run sudo apt-get update, it returns with an error for each file saying 'failed to fetch' <file> Could not resolve security.ubuntu.com anybody know why this happens?\n",
      "Assistant:\n",
      "Response:  It seems like there might be a typo in the path you're using to download packages from the Ubuntu repositories. Here's how you can fix it:\n",
      "\n",
      "1. Open a terminal.\n",
      "2. Type `sudo apt-get update` and press Enter.\n",
      "\n",
      "If you're still facing issues, try changing your search path in `/etc\n",
      "Confidence: 83.16 %\n"
     ]
    }
   ],
   "source": [
    "sample = distilled_dataset[23969]\n",
    "response, confidence = chat_with_model(sample['prompt'], return_confidence=True)\n",
    "# response = chat_with_model(sample['prompt'])\n",
    "print(\"Prompt:\", sample['prompt'])\n",
    "print(\"Response:\", response)\n",
    "print(\"Confidence:\", round(confidence * 100, 2), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
