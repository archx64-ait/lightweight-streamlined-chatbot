{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"training_student.ipynb\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the distilled dataset from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('sample/merged_distilled_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "distilled_dataset = Dataset.from_pandas(df)\n",
    "distilled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "STUDENT_MODEL = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, trust_remote_code=True)\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(STUDENT_MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "student_model = prepare_model_for_kbit_training(student_model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "student_model = get_peft_model(student_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Distilled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7588beefbbe54e1ea578cd4b7d5faa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946cae4be26746eca502e4c93b1266c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "    prompt = example.get('prompt') or ''\n",
    "    response = example.get('response') or ''\n",
    "    full_text = prompt + ' ' + response\n",
    "    return student_tokenizer(full_text, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "# tokenized_dataset = distilled_dataset.map(tokenize_fn)\n",
    "tokenized_dataset = distilled_dataset.map(tokenize_fn)\n",
    "tokenized_dataset = tokenized_dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_dataset[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "val_test_split = split_dataset['test'].train_test_split(test_size=0.5, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "valid_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing metrics & Memory profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "history = {'eval_loss': [], 'eval_ppl': [], 'gpu_mem': [], 'cpu_mem': []}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    loss = eval_pred.metrics[\"eval_loss\"]\n",
    "    ppl = torch.exp(torch.tensor(loss)).item()\n",
    "    history[\"eval_loss\"].append(loss)\n",
    "    history[\"eval_ppl\"].append(ppl)\n",
    "\n",
    "    gpu_mem = torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "    history[\"gpu_mem\"].append(gpu_mem)\n",
    "    history[\"cpu_mem\"].append(psutil.virtual_memory().percent)\n",
    "\n",
    "    return {\"perplexity\": ppl}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-22 13:16:48,432] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/tljh/user/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/qwen2-lora-distilled\",\n",
    "    run_name='student-qwen2-0.5B-lora-distilled',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    # num_train_epochs=5,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    # save_strategy=\"steps\",\n",
    "    # eval_strategy=\"steps\",\n",
    "    # eval_steps=100,\n",
    "    eval_strategy=\"no\",\n",
    "    save_safetensors=True,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=student_tokenizer,\n",
    "#     mlm=False\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    # data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst124974\u001b[0m (\u001b[33mbinit-ait\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-st124974/wandb/run-20250422_131652-9aqbodso</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/huggingface/runs/9aqbodso' target=\"_blank\">student-qwen2-0.5B-lora-distilled</a></strong> to <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">https://wandb.ai/binit-ait/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/huggingface/runs/9aqbodso' target=\"_blank\">https://wandb.ai/binit-ait/huggingface/runs/9aqbodso</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [218/218 07:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.382900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.350300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.356700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('models/qwen2-lora-distilled-final/tokenizer_config.json',\n",
       " 'models/qwen2-lora-distilled-final/special_tokens_map.json',\n",
       " 'models/qwen2-lora-distilled-final/vocab.json',\n",
       " 'models/qwen2-lora-distilled-final/merges.txt',\n",
       " 'models/qwen2-lora-distilled-final/added_tokens.json',\n",
       " 'models/qwen2-lora-distilled-final/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model(\"models/qwen2-lora-distilled-final\")\n",
    "student_tokenizer.save_pretrained(\"models/qwen2-lora-distilled-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.plot(history[\"eval_loss\"], label=\"Validation Loss\", linewidth=2, color='green')\n",
    "# plt.plot(history[\"eval_ppl\"], label=\"Validation Perplexity\", linewidth=2, color='orange')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Value\")\n",
    "# plt.title(\"Validation Loss & Perplexity\")\n",
    "# plt.legend()\n",
    "# plt.savefig(\"validation_metrics.png\")\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.plot(history[\"gpu_mem\"], label=\"GPU Memory (MB)\", linewidth=2, color='green')\n",
    "# plt.plot(history[\"cpu_mem\"], label=\"CPU Memory (%)\", linewidth=2, color='orange')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Usage\")\n",
    "# plt.title(\"Memory Usage During Training\")\n",
    "# plt.legend()\n",
    "# plt.savefig(\"memory_usage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='models/qwen2-lora-distilled-checkpoints/',\n",
    "#     per_device_train_batch_size=2,\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     gradient_accumulation_steps=8,\n",
    "#     num_train_epochs=10,\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "#     save_total_limit=2,\n",
    "#     save_strategy='epoch',\n",
    "#     evaluation_strategy='epoch',\n",
    "#     save_safetensors=True,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model='eval_loss',\n",
    "#     greater_is_better=False\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=student_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=valid_dataset,\n",
    "#     tokenizer=student_tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda-12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
